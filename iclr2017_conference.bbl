\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balzer et~al.(1991)Balzer, Takahashi, Ohta, and
  Kyuma]{balzer1991weight}
Wolfgang Balzer, Masanobu Takahashi, Jun Ohta, and Kazuo Kyuma.
\newblock Weight quantization in boltzmann machines.
\newblock \emph{Neural Networks}, 4\penalty0 (3):\penalty0 405--409, 1991.

\bibitem[Dundar \& Rose(1994)Dundar and Rose]{dundar1994effects}
Gunhan Dundar and Kenneth Rose.
\newblock The effects of quantization on multilayer neural networks.
\newblock \emph{IEEE transactions on neural networks/a publication of the IEEE
  Neural Networks Council}, 6\penalty0 (6):\penalty0 1446--1451, 1994.

\bibitem[Fahlman \& Lebiere(1989)Fahlman and Lebiere]{fahlman1989cascade}
Scott~E Fahlman and Christian Lebiere.
\newblock The cascade-correlation learning architecture.
\newblock 1989.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Warde-Farley, Mirza, Courville, and
  Bengio]{goodfellow2013maxout}
Ian~J Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, and Yoshua
  Bengio.
\newblock Maxout networks.
\newblock \emph{arXiv preprint arXiv:1302.4389}, 2013.

\bibitem[Han et~al.(2016)Han, Mao, and Dally]{deepcompression2016}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149v5}, 2016.

\bibitem[Hassibi \& Stork(1993)Hassibi and Stork]{hassibi1993second}
Babak Hassibi and David~G Stork.
\newblock \emph{Second order derivatives for network pruning: Optimal brain
  surgeon}.
\newblock Morgan Kaufmann, 1993.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{darkknowledge2015}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hoehfeld \& Fahlman(1992)Hoehfeld and Fahlman]{hoehfeld1992learning}
Markus Hoehfeld and Scott~E Fahlman.
\newblock Learning with limited numerical precision using the
  cascade-correlation algorithm.
\newblock \emph{IEEE Transactions on Neural Networks}, 3\penalty0 (4):\penalty0
  602--611, 1992.

\bibitem[LeCun \& Cortes(2010)LeCun and
  Cortes]{lecun-mnisthandwrittendigit-2010}
Yann LeCun and Corinna Cortes.
\newblock {MNIST} handwritten digit database.
\newblock 2010.

\bibitem[LeCun et~al.(1989)LeCun, Denker, Solla, Howard, and
  Jackel]{lecun1989optimal}
Yann LeCun, John~S Denker, Sara~A Solla, Richard~E Howard, and Lawrence~D
  Jackel.
\newblock Optimal brain damage.
\newblock In \emph{NIPs}, volume~89, 1989.

\bibitem[Mozer \& Smolensky(1989{\natexlab{a}})Mozer and
  Smolensky]{mozer1989skeletonization}
Michael~C Mozer and Paul Smolensky.
\newblock Skeletonization: A technique for trimming the fat from a network via
  relevance assessment.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  107--115, 1989{\natexlab{a}}.

\bibitem[Mozer \& Smolensky(1989{\natexlab{b}})Mozer and
  Smolensky]{mozer1989using}
Michael~C Mozer and Paul Smolensky.
\newblock Using relevance to reduce network size automatically.
\newblock \emph{Connection Science}, 1\penalty0 (1):\penalty0 3--16,
  1989{\natexlab{b}}.

\bibitem[Oland \& Raj(2015)Oland and Raj]{Anders2016quant}
Anders Oland and Bhiksha Raj.
\newblock Reducing communication overhead in distributed learning by an order
  of magnitude (almost).
\newblock In \emph{{IEEE} International Conference on Acoustics, Speech and
  Signal Processing}, pp.\  2219--2223, 2015.

\bibitem[Prabhavalkar et~al.(2016)Prabhavalkar, Alsharif, Bruguier, and
  McGraw]{prabhavalkar2016svd}
Rohit Prabhavalkar, Ouais Alsharif, Antoine Bruguier, and Ian McGraw.
\newblock On the compression of recurrent neural networks with an application
  to {LVCSR} acoustic modeling for embedded speech recognition.
\newblock \emph{arXiv preprint arXiv:1603.08042v2}, 2016.

\bibitem[Reed(1993)]{reed1993pruning}
Russell Reed.
\newblock Pruning algorithms-a survey.
\newblock \emph{Neural Networks, IEEE Transactions on}, 4\penalty0
  (5):\penalty0 740--747, 1993.

\bibitem[Segee \& Carter(1991)Segee and Carter]{segee1991fault}
Bruce~E Segee and Michael~J Carter.
\newblock Fault tolerance of pruned multilayer networks.
\newblock In \emph{Neural Networks, 1991., IJCNN-91-Seattle International Joint
  Conference on}, volume~2, pp.\  447--452. IEEE, 1991.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\end{thebibliography}
