\section{Conclusions \& Future Work}
%\subsection{Conclusions}
Pruning neurons (instead of pruning individual weights) in a pre-trained neural network without seeing a major loss in performance is not only possible but also enables compressing networks to 40-80\% of their original size, which is of great importance in constrained memory environments like embedded devices. This fact is established through the experiments using the brute force criterion, which if made computationally viable (through parallelization) or approximated more efficiently than the Taylor Series based methods discussed in this paper, can prove to be a useful compression tool. It would also be interesting to see how these methods perform on deeper networks and on some other popular and real world datasets. Also, as mentioned in Section \ref{sec2}, we have not considered all possible combinations of neuron interdependence in this work due to the in-feasibility of implementation and have always pruned one neuron at a time. Even though the greedy evaluation of such combinations is highly prohibitive, it is not entirely implausible to think that algorithms can be developed for it in the future. That would help in truly tapping into the power of these interconnections and hopefully lead to impressive performance results.

The experiments on the visualization of error surfaces and pruning decisions concretely establish the fact that not all neurons in a network contribute to its performance in the same way. This confirms the idea put forth by \cite{mozer1989using} that learning representation is not distributed uniformly across neurons. Neural networks use a few neurons to learn the function approximation, and the remaining neurons cooperate to cancel out each other's effects. This is also a strong indication of the fact that once training is done, bigger networks do not hold an advantage over smaller ones, which is similar to the idea put forth by \cite{darkknowledge2015} in their work on ensemble learning techniques.

%It might also be interesting to see if the Cascade Correlation architecture arrives at the same number of final neurons as the pruning technique discussed in this paper.
