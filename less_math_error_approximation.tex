\subsubsection{Linear Approximation Approach}
We can use equation \ref{eq:ts3} to get the linear error approximation of the change in error due to the $k$th neuron being turned off and represent it as $\Delta E_{k}^1$ as follows:

\begin{align}
\Delta E_{k}^1 = - O_k\cdot \left.\pdv{E}{O}\right|_{O_k},
\end{align}

The derivative term above is the first-order gradient which represents the change in error with respect to the output a given neuron. This term can be collected during back-propagation. As we shall see further in this section, linear approximations are not reliable indicators of change in error but they provide us with an interesting basis for comparison with the other methods discussed in this paper.

\subsubsection{Quadratic Approximation Approach}
As above, we can use equation \ref{eq:ts3} to get the quadratic error approximation of the change in error due to the $k$th neuron being turned off and represent it as $\Delta E_{k}^2$ as follows:

\begin{align}
\Delta E_{k}^2 =  - O_k\cdot \left.\pdv{E}{O}\right|_{O_k} + 0.5\cdot O_k^2\cdot \left.\pdv[2]{E}{O}\right|_{O_k},
\end{align}

The additional second-order gradient term appearing above represents the quadratic change in error with respect to the output of a given neuron. This term can be generated by performing back-propagation using second order derivatives. Collecting these quadratic gradients involves some non-trivial mathematics, the entire step-by-step derivation procedure of which is provided in the Supplementary Material as an Appendix.