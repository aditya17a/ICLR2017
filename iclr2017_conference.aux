\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{mozer1989skeletonization}
\citation{segee1991fault}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec1}{{1}{1}{Introduction}{section.1}{}}
\citation{mozer1989using}
\citation{fahlman1989cascade}
\citation{reed1993pruning}
\citation{chauvin1990generalization}
\citation{baum1989size}
\citation{srivastava2014dropout}
\citation{goodfellow2013maxout}
\citation{fahlman1989cascade}
\citation{balzer1991weight}
\citation{dundar1994effects}
\citation{hoehfeld1992learning}
\citation{prabhavalkar2016compression}
\citation{Anders2016quant}
\citation{deepcompression2016}
\@writefile{toc}{\contentsline {section}{\numberline {2}Literature Review}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Non-Pruning Based Generalization \& Compression Techniques}{2}{subsection.2.1}}
\citation{mozer1989skeletonization}
\citation{lecun1989optimal}
\citation{hassibi1993second}
\citation{segee1991fault}
\citation{mozer1989using}
\citation{mozer1989using}
\citation{mozer1989skeletonization}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Pruning Techniques}{3}{subsection.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Pruning Neurons to Shrink Neural Networks}{3}{section.3}}
\newlabel{sec2}{{3}{3}{Pruning Neurons to Shrink Neural Networks}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Brute Force Removal Approach}{4}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Taylor Series Representation of Error}{4}{subsection.3.2}}
\newlabel{eq:ts1}{{1}{4}{Taylor Series Representation of Error}{equation.3.1}{}}
\newlabel{eq:ts3}{{2}{4}{Taylor Series Representation of Error}{equation.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.1}Linear Approximation Approach}{4}{subsubsection.3.2.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.2.2}Quadratic Approximation Approach}{4}{subsubsection.3.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces The intuition behind neuron pruning decision.}}{5}{figure.1}}
\newlabel{fig:intuition}{{1}{5}{The intuition behind neuron pruning decision}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Proposed Pruning Algorithm}{5}{subsection.3.3}}
\citation{lecun-mnisthandwrittendigit-2010}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Algorithm I: Single Overall Ranking}{6}{subsubsection.3.3.1}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Single Overall Ranking}}{6}{algocf.1}}
\newlabel{algo1}{{1}{6}{Algorithm I: Single Overall Ranking}{algocf.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Algorithm II: Iterative Re-Ranking}{6}{subsubsection.3.3.2}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Iterative Re-Ranking}}{6}{algocf.2}}
\newlabel{algo2}{{2}{6}{Algorithm II: Iterative Re-Ranking}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Results}{6}{section.4}}
\citation{mozer1989skeletonization}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Pruning a 1-Layer Network}{7}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.1}Single Overall Ranking Algorithm}{7}{subsubsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network using The Single Overall Ranking algorithm (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{7}{figure.2}}
\newlabel{fig:mnist-single-ranking-single-layer}{{2}{7}{Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network using The Single Overall Ranking algorithm (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.2}Iterative Re-Ranking Algorithm}{7}{subsubsection.4.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.1.3}Visualization of Error Surface \& Pruning Decisions}{7}{subsubsection.4.1.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network the Iterative Re-ranking algorithm (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{8}{figure.3}}
\newlabel{fig:mnist-re-ranking-single-layer}{{3}{8}{Degradation in squared error (left) and classification accuracy (right) after pruning a single-layer network the Iterative Re-ranking algorithm (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the brute force criterion; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{8}{figure.4}}
\newlabel{fig:mnist-gt-single-layer}{{4}{8}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the brute force criterion; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 1st order Taylor Series error approximation criterion; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{8}{figure.5}}
\newlabel{fig:mnist-gt-single-layer}{{5}{8}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 1st order Taylor Series error approximation criterion; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 2nd order Taylor Series error approximation criterion; (\textbf  {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}}{9}{figure.6}}
\newlabel{fig:mnist-gt-single-layer}{{6}{9}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 2nd order Taylor Series error approximation criterion; (\textbf {Network:} 1 layer, 100 neurons, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.998)}{figure.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Pruning A 2-Layer Network}{10}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.1}Single Overall Ranking Algorithm}{10}{subsubsection.4.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network using the Single Overall Ranking algorithm; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{10}{figure.7}}
\newlabel{fig:mnist-single-ranking-double-layer}{{7}{10}{Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network using the Single Overall Ranking algorithm; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.7}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.2}Iterative Re-Ranking Algorithm}{10}{subsubsection.4.2.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network using the Iterative Re-ranking algorithm; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{10}{figure.8}}
\newlabel{fig:mnist-re-ranking-double-layer}{{8}{10}{Degradation in squared error (left) and classification accuracy (right) after pruning a 2-layer network using the Iterative Re-ranking algorithm; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.2.3}Visualization of Error Surface \& Pruning Decisions}{11}{subsubsection.4.2.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the brute force criterion; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{11}{figure.9}}
\newlabel{fig:mnist-gt-double-layer}{{9}{11}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the brute force criterion; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 1st order Taylor Series error approximation criterion; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{11}{figure.10}}
\newlabel{fig:mnist-g1-double-layer}{{10}{11}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 1st order Taylor Series error approximation criterion; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Experiments on Toy Datasets}{12}{subsection.4.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 2nd order Taylor Series error approximation criterion; (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}}{12}{figure.11}}
\newlabel{fig:mnist-g2-double-layer}{{11}{12}{Error surface of the network output in log space (left) and in real space (right) with respect to each candidate neuron chosen for removal using the 2nd order Taylor Series error approximation criterion; (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 1.000)}{figure.11}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Degradation in squared error after pruning a 2-layer network using the Single Pass Algorithm (left) and the Iterative Re-ranking algorithm(right) on toy "diamond" shape dataset (above) and toy "random shape" dataset (below); (\textbf  {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.992(diamond); 0.986(random shape)}}{13}{figure.12}}
\newlabel{fig:diamond}{{12}{13}{Degradation in squared error after pruning a 2-layer network using the Single Pass Algorithm (left) and the Iterative Re-ranking algorithm(right) on toy "diamond" shape dataset (above) and toy "random shape" dataset (below); (\textbf {Network:} 2 layers, 50 neurons/layer, 10 outputs, logistic sigmoid activation, starting test accuracy: 0.992(diamond); 0.986(random shape)}{figure.12}{}}
\citation{mozer1989using}
\citation{darkknowledge2015}
\bibdata{iclr2017_conference}
\bibcite{balzer1991weight}{{1}{1991}{{Balzer et~al.}}{{Balzer, Takahashi, Ohta, and Kyuma}}}
\bibcite{baum1989size}{{2}{1989}{{Baum \& Haussler}}{{Baum and Haussler}}}
\bibcite{chauvin1990generalization}{{3}{1990}{{Chauvin}}{{}}}
\bibcite{dundar1994effects}{{4}{1994}{{Dundar \& Rose}}{{Dundar and Rose}}}
\bibcite{fahlman1989cascade}{{5}{1989}{{Fahlman \& Lebiere}}{{Fahlman and Lebiere}}}
\bibcite{goodfellow2013maxout}{{6}{2013}{{Goodfellow et~al.}}{{Goodfellow, Warde-Farley, Mirza, Courville, and Bengio}}}
\bibcite{deepcompression2016}{{7}{2016}{{Han et~al.}}{{Han, Mao, and Dally}}}
\bibcite{hassibi1993second}{{8}{1993}{{Hassibi \& Stork}}{{Hassibi and Stork}}}
\bibcite{darkknowledge2015}{{9}{2015}{{Hinton et~al.}}{{Hinton, Vinyals, and Dean}}}
\bibcite{hoehfeld1992learning}{{10}{1992}{{Hoehfeld \& Fahlman}}{{Hoehfeld and Fahlman}}}
\bibcite{lecun-mnisthandwrittendigit-2010}{{11}{2010}{{LeCun \& Cortes}}{{LeCun and Cortes}}}
\bibcite{lecun1989optimal}{{12}{1989}{{LeCun et~al.}}{{LeCun, Denker, Solla, Howard, and Jackel}}}
\bibcite{mozer1989skeletonization}{{13}{1989{a}}{{Mozer \& Smolensky}}{{Mozer and Smolensky}}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Conclusions \& Future Work}{14}{section.5}}
\bibcite{mozer1989using}{{14}{1989{b}}{{Mozer \& Smolensky}}{{Mozer and Smolensky}}}
\bibcite{Anders2016quant}{{15}{2015}{{Oland \& Raj}}{{Oland and Raj}}}
\bibcite{prabhavalkar2016compression}{{16}{2016}{{Prabhavalkar et~al.}}{{Prabhavalkar, Alsharif, Bruguier, and McGraw}}}
\bibcite{reed1993pruning}{{17}{1993}{{Reed}}{{}}}
\bibcite{segee1991fault}{{18}{1991}{{Segee \& Carter}}{{Segee and Carter}}}
\bibcite{srivastava2014dropout}{{19}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}}}
\bibstyle{iclr2017_conference}
\@writefile{toc}{\contentsline {section}{\numberline {A}Second Derivative Back-Propagation}{16}{appendix.A}}
\newlabel{apd:first}{{A}{16}{Second Derivative Back-Propagation}{appendix.A}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces A computational graph of a simple feed-forward network illustrating the naming of different variables, where $\sigma (\cdot )$ is the nonlinearity, MSE is the mean-squared error cost function and $E$ is the overall loss.}}{16}{figure.13}}
\newlabel{fig:comp_graph}{{13}{16}{A computational graph of a simple feed-forward network illustrating the naming of different variables, where $\sigma (\cdot )$ is the nonlinearity, MSE is the mean-squared error cost function and $E$ is the overall loss}{figure.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}First and Second Derivatives}{16}{subsection.A.1}}
\newlabel{cost_func_derivative}{{6}{16}{First and Second Derivatives}{equation.A.6}{}}
\newlabel{cost_func_2nd_derivative}{{7}{16}{First and Second Derivatives}{equation.A.7}{}}
\newlabel{sigmoid_derivative}{{8}{16}{First and Second Derivatives}{equation.A.8}{}}
\newlabel{sigmoid_2nd_derivative}{{9}{16}{First and Second Derivatives}{equation.A.9}{}}
\newlabel{dedx}{{25}{17}{First and Second Derivatives}{equation.A.25}{}}
\newlabel{d2edx2}{{31}{18}{First and Second Derivatives}{equation.A.31}{}}
\newlabel{dxdc}{{34}{18}{First and Second Derivatives}{equation.A.34}{}}
\newlabel{dedc}{{36}{18}{First and Second Derivatives}{equation.A.36}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.1}Summary Of Output Layer Derivatives}{19}{subsubsection.A.1.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.2}Hidden Layer Derivatives}{20}{subsubsection.A.1.2}}
\newlabel{dcdo}{{53}{20}{Hidden Layer Derivatives}{equation.A.53}{}}
\newlabel{dedo_general}{{55}{20}{Hidden Layer Derivatives}{equation.A.55}{}}
\newlabel{d2edo2}{{65}{21}{Hidden Layer Derivatives}{equation.A.65}{}}
\newlabel{de2do2_general}{{66}{21}{Hidden Layer Derivatives}{equation.A.66}{}}
\newlabel{dedx_general}{{67}{21}{Hidden Layer Derivatives}{equation.A.67}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {A.1.3}Summary Of Hidden Layer Derivatives}{22}{subsubsection.A.1.3}}
