\section{Introduction}\label{sec1}

In this work we propose and evaluate a novel algorithm for pruning whole neurons from a trained neural network without any re-training and examine its performance compared to simpler methods. We then analyze the kinds of errors made by our algorithm and use this to launch an investigation into the fundamental nature of learning representations in neural networks. We find that our results corroborate a seemingly forgotten 27-year old observation by \cite{mozer1989skeletonization} concerning the nature of neural network learning. This observation is best summarized in a quotation from \cite{segee1991fault} on the notion of fault-tolerance in multilayer perception networks:

\begin{quotation}
Contrary to the belief widely held, multilayer networks are \textit{not} inherently fault tolerant. In fact, the loss of a single weight is frequently sufficient to completely disrupt a learned function approximation. Furthermore, having a large number of weights \textit{does not seem} to improve fault tolerance. [Emphasis added]
\end{quotation}

Essentially, \cite{mozer1989using} observed that the behavior of neural networks during training is not to distribute the learning representation evenly or even equitably across hidden units. What actually happens is that a few, elite neurons learn an approximation of the input-output function, and the remaining units simply learn a complex interdependence function which cancels out their respective influence. Furthermore, assuming enough units exist to learn the function in question, increasing the number of parameters does not increase the richness or robustness of the learned approximation, but rather increases the number of noisy parameters to cancel. This is evinced by the fact that in many cases, whole neurons can be removed from a network with minimal to no impact on the output approximation. In this sense, there are few bipartisan units in a trained network. A unit is typically either part of the input-output function approximation, or it is part of an elaborate noise cancellation task force. Assuming this is true, most of the compute-time spent training a neural network is likely occupied by this arguably wasteful procedure of silencing superfluous parameters, and pruning is thus part of a necessary treatment procedure to ``trim the fat''. 

We observed this very same phenomenon in our experiments, and this is the motivation behind our decision to evaluate our algorithms on the criteria of their ability to trim neurons without any re-training. As \cite{fahlman1989cascade} discuss, the ``herd effect'' and ``moving target'' problems in back-propagation learning simply cause the remaining units in a network to shift course and What we discovered is that predicting the behavior of a network when a unit is to be pruned is very difficult, and 


\section{Literature Review}
Pruning algorithms, as comprehensively surveyed by \cite{reed1993pruning}, are a useful set of heuristics designed to identify and remove elements from a neural network which are either redundant or do not significantly contribute to the output of the network. This is motivated by the observed tendency of neural networks to overfit to the idiosyncrasies of their training data given too many trainable parameters or too few input patterns from which to generalize, as stated by \cite{chauvin1990generalization}. 

Network architecture design and hyperparameter selection are inherently difficult tasks typically approached using a few well-known rules of thumb, e.g. various weight initialization procedures, choosing the width and number of layers, different activation functions, learning rates, momentum, etc. Some of this ``black art'' appears unavoidable. For problems which cannot be solved using linear threshold units alone, \cite{baum1989size} demonstrate that there is no way to precisely determine the appropriate size of a neural network a priori given any random set of training instances. Using too few neurons seems to inhibit learning, and so in practice it is common to attempt to over-parameterize networks initially using a large number of hidden units and weights, and then prune or compress them afterwards if necessary. Of course, as the old saying goes, there's more than one way to skin a neural network. 

\subsection{Non-Pruning Based Generalization \& Compression Techniques}

The generalization behavior of neural networks has been well studied, and apart from pruning algorithms many heuristics have been used to avoid overfitting, such as dropout (\cite{srivastava2014dropout}), maxout (\cite{goodfellow2013maxout}), and cascade correlation (\cite{fahlman1989cascade}), among others. Of course, while cascade correlation specifically tries to construct of minimal networks, many techniques to improve network generalization do not explicitly attempt to reduce the total number of parameters or the memory footprint of a trained network per se.  

Model compression often has benefits with respect to generalization performance and the portability of neural networks to operate in memory-constrained or embedded environments. Without explicitly removing parameters from the network, weight quantization allows for a reduction in the number of bytes used to represent each weight parameter, as investigated by \cite{balzer1991weight}, \cite{dundar1994effects}, and \cite{hoehfeld1992learning}. 

A recently proposed method for compressing recurrent neural networks (\cite{prabhavalkar2016compression}) uses the singular values of a trained weight matrix as basis vectors from which to derive a compressed hidden layer. Some other recent works like \cite{Anders2016quant} have tried successfully to achieve compression through weight quantization followed by an encoding step while others such as \cite{deepcompression2016} have tried to expand on this by adding weight-pruning as a preceding step to quantization and encoding. 

In summary, we can say that there are many different ways to improve network generalization by altering the training procedure, the objective error function, or by using compressed representations of the network parameters.

\subsection{Pruning Techniques}

If we wanted to continually shrink a network to its absolute minimal size, we might accomplish this using any number of off-the-shelf pruning algorithms, such as Skeletonization (\cite{mozer1989skeletonization}), Optimal Brain Damage (\cite{lecun1989optimal}), or later variants such as Optimal Brain Surgeon (\cite{hassibi1993second}). In fact, we borrow much of our inspiration from these antecedent algorithms, with one major variation: Instead of pruning individual weights, we prune entire neurons, thereby eliminating all of their incoming and outgoing weight parameters in one go, resulting in more memory saved, faster.

Scoring and ranking individual weight parameters in a large network is computationally expensive, and generally speaking the removal of a single weight from a large network is a drop in the bucket in terms of reducing a network's core memory footprint.  We argue that pruning neurons instead of weights is more efficient computationally as well as practically in terms of quickly reaching a target reduction in memory size. Our approach also attacks the angle of giving downstream applications a realistic expectation of the minimal increase in error resulting from the removal of a specified percentage of neurons. Such trade-offs are unavoidable, but performance impacts can be limited if a principled approach is used to find candidate neurons for removal. 

Too many free parameters in a Neural Network lead to overfitting. Regardless of the number of weights used in a given network, as \cite{segee1991fault} assert, the representation of a learned function approximation is almost never evenly distributed over the hidden units, and the removal of any single hidden unit at random can actually result in a total network fault. \cite{mozer1989using} suggest that only a subset of the hidden units in a neural network actually latch on to the invariant or generalizing properties of the training inputs, and the rest learn to either mutually cancel each other out or begin over-fitting to the noise in the data. We leverage this idea in the current work to rank all neurons in pre-trained networks based on their effective contributions to the overall performance. We then remove the unnecessary neurons to reduce the network's footprint. Through our experiments we not only concretely validate the theory put forth by \cite{mozer1989using} but we also successfully build on  it to prune networks to 40 to 60 \% of their original size without any major loss in performance.
