\begin{abstract}
How much can pruning algorithms teach us about the fundamentals of learning representations in neural networks? A lot, it turns out. Neural network model compression has become a topic of great interest in recent years, and many algorithms have been proposed to address this problem. We reviewed the historical literature and derived a novel algorithm to prune whole neurons from optimally trained networks using a second-order Taylor method. We then set about testing its performance and analyzing the quality of the decisions it made. As a baseline for comparison we used a first-order Taylor method and an exhaustive brute-force pruning algorithm. Our proposed algorithm worked well compared to a first-order method, but not nearly as well as the brute-force method. While this result may seem a bit obvious, the performance of the brute-force pruning baseline was so impressive as to be uncanny. We discovered that there is in fact a way, however computationally intractable, to serially prune 40-60\% of the neurons in a trained network \textit{with minimal effect on the learning representation and no requirement of re-training.}


We propose and investigate several methods for pruning artificial neural networks to operate in constrained memory environments such as mobile or embedded devices. Unlike traditional methods that compress networks by pruning their weights, we focus on pruning entire neurons, thereby eliminating all the incoming and outgoing weights from a pruned node resulting in more memory saved. All the proposed methods prune neurons based off a 'ranked list' of their contribution to overall network performance. Each method proposed uses a different way of creating this ranked list. We evaluate these methods and look into their strengths and limitations. We also evaluate 2 different algorithms to perform pruning on these created lists and compare their performance. In doing so, we also look into the correlation between neurons in a trained network and their learning representations, using which we try to answer an age old question: are all the neurons in a network the same? We argue that within this answer lies the perfect method for pruning that allows for the optimal trade-off in network size versus accuracy in order to operate within the memory constraints of a particular device or application environment. 
\end{abstract}