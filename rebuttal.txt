We thank the reviewers for the useful feedback, and for their notes on typos & figures. We have bucketed common points from reviewers in our responses below.

>Response_1 (Reviewers 2, 3, 4): 
>Numerical comparisons with the state-of-the-art methods/recent related techniques not given, e.g. Hinton distillation and the Hashing Trick (ICML 2015).

RE: Under certain motivational assumptions, it is understandable to demand benchmarking comparisons against state-of-the-art methods, but this may be missing the fundamental purpose of the present research. Our investigation is intended less to propose a competing alternative to existing pruning techniques and more so to shed light on the limitations of generally accepted approaches to pruning and the degree to which increased numbers of parameters affect learning representations in neural networks. The Hashing Trick, proposed by Chen et al. is essentially a lookup table for quantized weight parameters, and Hinton’s distillation technique focuses on ensemble learning to improve generalization. Neither of these methods is explicitly concerned with network pruning per se, and they address different practical concerns with different assumptions about the nature of learning representations in general. 

To start, we examined the literature for numerical methods to approximate the importance of network elements, and the widely-cited 1st & 2nd order techniques proposed by Mozer, LeCun, Hassibi, Stork, et al. provided our initial inspiration. We extended this to the novel application of pruning whole neurons and discovered that both 1st & 2nd order Taylor methods are flawed when compared to a brute force approach. This is the jumping off point for our research in terms of key insights. Our investigation demonstrated that it is possible, without re-training, to eliminate 40-80% of the neurons in an optimally trained network in a serial fashion. From this we propound 2 significant conclusions. 

First, the empirical comparison of pruning methods and lack of re-training in our experiments demonstrates that learning representations are not distributed over all parameters and a substantial portion of network elements learn only to cancel each other’s influence on the network output. In cases where the network did not achieve optimal performance during training, the removal of the first few neurons actually improves performance, suggesting that valuable training time is being squandered on redundant neurons which could have simply been pruned outright. We have thus provided substantial evidence for Mozer & Smolensky’s 1989 hypothesis on the nature of neural network learning representations using the well-studied MNIST dataset, suggesting that arbitrarily increasing the number of network parameters does not enrich the learning representation. This fundamentally disagrees with the premise that we should compress networks while preserving the overall number of parameters, or attempt to coax networks to maximize parameter utilization. We cite existing work on compression via singular-value-based techniques but these methods typically require re-training.

Second, determining which neurons are doing the heavy lifting is hard. The relationship between network elements and output fault-sensitivity is a highly nonlinear mapping which is poorly approximated using 1st & 2nd order approaches. Our experiments and graphical investigations of the network error surface show that the 1st & 2nd order methods are making well-informed decisions insofar as these methods can approximate the change in error resulting from the removal of a network element, but ultimately we find these choices are far from ideal and often cause network faults. 

>Response_2 (Reviewers 2, 4): 
>The 2nd order Taylor approximation does not work for the up-to-date techniques such as ReLUs. The settings of networks in the experiments should be improved.
>The experiment is conducted only on the MNIST data set in which small network can generalize. 

RE: As mentioned above, the main contribution of this work is to demonstrate the feasibility of pruning entire neurons from trained networks, and offer novel insight on learning representations. The brute-force method shows that pruning a considerable part of the network without re-training or losing accuracy is possible. We use Taylor methods to approximate the results achieved by the brute-force method but this is not an ideal solution to the problem, as we discuss. 

The 2nd order approximation technique will not work for ReLU networks because ReLUs do not have a 2nd derivative, unless we use the soft-plus function as a continuous approximation. Furthermore, due to the fact that we are approximating the error surface of a network element with respect to the output using a parabola, if there is no useful parabola to approximate this relationship, then the method breaks down. The derivatives of the activation function are simply parameters of the Taylor series. It doesn’t cease to be a parabolic approximation or become more effective if we use a different doubly-differentiable activation function. 

Also, all experiments were necessarily carried out on optimally trained networks, so there is no way to improve them. We derived the algorithm assuming the well-studied sigmoid activation function. Furthermore, the MNIST dataset is a de-facto standard for demonstrating the potential of new techniques. A different dataset, task, activation function, or network architecture could make the results less interpretable. 

>Response_3 (Reviewer 4):
>The best setting is Iterative Re-ranking with Brute Force removal which is too expensive.

RE: The brute-force method is highly parallelizable, so time complexity is not necessarily a deal-breaker. Our focus is the proof of concept, and we intend to investigate potential speedups in future work. If pruning need only be done once, this is potentially acceptable.
